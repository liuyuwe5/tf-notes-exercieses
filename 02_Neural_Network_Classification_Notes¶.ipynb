{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Neural_Network_Classification_Notes¬∂.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyND+mpYfZH6j+uE24LTQwyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oolonglilfox/tf-notes-exercieses/blob/main/02_Neural_Network_Classification_Notes%C2%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. tf.keras.losses.BinaryCrossentropy & tf.keras.losses.CategoricalCrossentropy & tf.keras.losses.SparseCategoricalCrossentropy\n",
        "2. Why use Softmax, Sigmoid as output layer activition, and ReLU be hidden layers' activition?\n",
        "3. Activation in Classification vs. Regression.\n",
        "4. layers.Flatten vs. layers.Dense"
      ],
      "metadata": {
        "id": "FTCJl24bk-z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. BinaryCrossentropy vs. CategoricalCrossentropy  vs. SparseCategoricalCrossentropy\n",
        "It all depends on the type of classification problem you are dealing with. There are three main categories \n",
        "- binary classification (two target classes),\n",
        "- multi-class classification (more than two exclusive targets),\n",
        "- multi-label classification (more than two non exclusive targets), in which multiple target classes can be on at the same time.\n",
        "\n",
        "In the first case, binary cross-entropy should be used and targets should be encoded as one-hot vectors.\n",
        "\n",
        "In the second case, categorical cross-entropy should be used and targets should be encoded as one-hot vectors.\n",
        "\n",
        "In the last case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. Each output neuron (or unit) is considered as a separate random binary variable, and the loss for the entire vector of outputs is the product of the loss of single binary variables. Therefore it is the product of binary cross-entropy for each single output unit.\n",
        "\n",
        "The binary cross-entropy is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "         \\sum^{C}_c=y_c(\\mathbf{x}, \\mathbf{w}_c)^{t_c}(1-y_c(\\mathbf{x}, \\mathbf{w}_c))^{1-t_c}\n",
        "    \\end{align}\n",
        "and categorical cross-entropy is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "         \\sum^{C}_c=y_c(\\mathbf{x}, \\mathbf{w}_c)^{t_c}\n",
        "    \\end{align}\n",
        "\n",
        "Where ùë™ is the index running over the number of classes ùë™.\n",
        "\n",
        "Another explanation:\n",
        "\n",
        "- binary_crossentropy: Used as a loss function for binary classification model. The binary_crossentropy function computes the cross-entropy loss between true labels and predicted labels.\n",
        "- categorical_crossentropy: Used as a loss function for multi-class classification model where there are two or more output labels. The output label is assigned one-hot category encoding value in form of 0s and 1. The output label, if present in integer form, is converted into categorical encoding using keras.utils to_categorical method.\n",
        "- sparse_categorical_crossentropy: Used as a loss function for multi-class classification model where the output label is assigned integer value (0, 1, 2, 3‚Ä¶). This loss function is mathematically same as the categorical_crossentropy. It just has a different interface.\n",
        "\n"
      ],
      "metadata": {
        "id": "13TjOQ7pboK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Why use Softmax, Sigmoid as output layer activition, and ReLU be hidden layers' activition?\n",
        "\n",
        "Output layer:\n",
        "\n",
        "- [Different activition function and visualization](https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5)\n",
        "- [Soft and sigmoid function for the output layer](https://stackoverflow.com/questions/41409248/softmax-and-sigmoid-function-for-the-output-layer)\n",
        "- [Multi-label vs. Multi-class Classification: Sigmoid vs. Softmax](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)\n",
        "\n",
        "\n",
        "Hidden layer (relu):\n",
        "- The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers.\n",
        "\n",
        "- It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh. Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or ‚Äúdead‚Äù units.\n",
        "- [How to Choose an Activation Function for Deep Learning](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/#:~:text=The%20rectified%20linear%20activation%20function,such%20as%20Sigmoid%20and%20Tanh)"
      ],
      "metadata": {
        "id": "HC_iiUwZoz_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Activation in Classification vs. Regression.\n",
        "- [Deep Learning: Which Loss and Activation Functions should I use?](\n",
        "https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8)"
      ],
      "metadata": {
        "id": "NNj3atrk3n29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. layers.Flatten vs. layers.Dense\n",
        "- [What is the role of \"Flatten\" in Keras?](https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras)\n",
        "- [When to use Dense, Conv1/2D, Dropout, Flatten, and all the other layers?](https://datascience.stackexchange.com/questions/44124/when-to-use-dense-conv1-2d-dropout-flatten-and-all-the-other-layers)\n",
        "- [A Complete Understanding of Dense Layers in Neural Networks](https://datascience.stackexchange.com/questions/44124/when-to-use-dense-conv1-2d-dropout-flatten-and-all-the-other-layers)"
      ],
      "metadata": {
        "id": "-n-gBKYs3noP"
      }
    }
  ]
}